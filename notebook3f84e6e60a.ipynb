{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22c4267",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-07T15:58:17.315660Z",
     "iopub.status.busy": "2026-01-07T15:58:17.315415Z",
     "iopub.status.idle": "2026-01-07T15:58:27.656529Z",
     "shell.execute_reply": "2026-01-07T15:58:27.655883Z"
    },
    "papermill": {
     "duration": 10.346188,
     "end_time": "2026-01-07T15:58:27.658262",
     "exception": false,
     "start_time": "2026-01-07T15:58:17.312074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.ndimage import median_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8133fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T15:58:27.663064Z",
     "iopub.status.busy": "2026-01-07T15:58:27.662692Z",
     "iopub.status.idle": "2026-01-07T15:58:27.718670Z",
     "shell.execute_reply": "2026-01-07T15:58:27.717887Z"
    },
    "papermill": {
     "duration": 0.059983,
     "end_time": "2026-01-07T15:58:27.720146",
     "exception": false,
     "start_time": "2026-01-07T15:58:27.660163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION \n",
    "# ==========================================\n",
    "TRAIN_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos'\n",
    "TEST_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\n",
    "OUTPUT_PATH = '/kaggle/working/submission.csv'\n",
    "\n",
    "#Set hyperparameters\n",
    "BATCH_SIZE = 96\n",
    "IMAGE_SIZE = 96\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 0.004\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5079557a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T15:58:27.725266Z",
     "iopub.status.busy": "2026-01-07T15:58:27.724860Z",
     "iopub.status.idle": "2026-01-07T15:58:27.733564Z",
     "shell.execute_reply": "2026-01-07T15:58:27.732858Z"
    },
    "papermill": {
     "duration": 0.012853,
     "end_time": "2026-01-07T15:58:27.734898",
     "exception": false,
     "start_time": "2026-01-07T15:58:27.722045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# DATASET\n",
    "# ==========================================\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, return_id=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.return_id = return_id\n",
    "        self.samples = []\n",
    "        # Walk through the directory to find all frames\n",
    "        # Assuming structure: root/01/frame_00001.jpg        \n",
    "        video_folders = sorted(os.listdir(root_dir))\n",
    "        \n",
    "        for vid_folder in video_folders:\n",
    "            vid_path = os.path.join(root_dir, vid_folder)\n",
    "            if not os.path.isdir(vid_path):\n",
    "                continue\n",
    "            # Get all image files in the video folder\n",
    "            frames = sorted(glob.glob(os.path.join(vid_path, '*.*')))\n",
    "            \n",
    "            for frame_path in frames:\n",
    "                # parsing ID\n",
    "                # Folder: \"01\" -> 1\n",
    "                # File: \"frame_00001.jpg\" -> 1\n",
    "                try:\n",
    "                    video_id = int(vid_folder)\n",
    "                    filename = os.path.basename(frame_path)\n",
    "                    # Split 'frame_00001.jpg' -> '00001' -> 1\n",
    "                    frame_num = int(filename.split('_')[-1].split('.')[0])                  \n",
    "                    row_id = f\"{video_id}_{frame_num}\"\n",
    "                    self.samples.append((frame_path, row_id))\n",
    "                except ValueError:\n",
    "                    continue # Skip files that don't match pattern\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, row_id = self.samples[idx]\n",
    "        # Load Image\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        #return image with ID if requested (for inference), otherwise return just the image (for training)\n",
    "        if self.return_id:\n",
    "            return image, row_id\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84dddc90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T15:58:27.739851Z",
     "iopub.status.busy": "2026-01-07T15:58:27.739365Z",
     "iopub.status.idle": "2026-01-07T15:58:27.751224Z",
     "shell.execute_reply": "2026-01-07T15:58:27.750484Z"
    },
    "papermill": {
     "duration": 0.015944,
     "end_time": "2026-01-07T15:58:27.752688",
     "exception": false,
     "start_time": "2026-01-07T15:58:27.736744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# MODEL \n",
    "# ==========================================\n",
    "class MultiHeadSpatialAttention(nn.Module):\n",
    "    #Multi-head attention to capture different anomaly patterns\n",
    "    def __init__(self, in_channels, num_heads=3):\n",
    "        super(MultiHeadSpatialAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Multiple attention heads for different anomaly patterns\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, 1, kernel_size=1) for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        # Learnable fusion weights\n",
    "        self.fusion = nn.Conv2d(num_heads, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Generate multiple attention maps\n",
    "        attention_maps = []\n",
    "        for head in self.heads:\n",
    "            att = torch.sigmoid(head(x))\n",
    "            attention_maps.append(att)\n",
    "        \n",
    "        # Stack and fuse\n",
    "        stacked = torch.cat(attention_maps, dim=1)\n",
    "        fused_attention = torch.sigmoid(self.fusion(stacked))\n",
    "        \n",
    "        return x * fused_attention, fused_attention\n",
    "\n",
    "class MultiHeadBottleneck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadBottleneck, self).__init__()\n",
    "\n",
    "        # encoder architecture\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(0.45),\n",
    "        )\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(0.45),\n",
    "        )\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(0.40),\n",
    "        )\n",
    "        \n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(0.35),\n",
    "        )\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadSpatialAttention(128, num_heads=3)\n",
    "        \n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(128, 32, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        \n",
    "        e4_attended, attention_map = self.attention(e4)\n",
    "        \n",
    "        encoded = self.bottleneck(e4_attended)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return decoded, encoded, attention_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f6a7f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T15:58:27.757176Z",
     "iopub.status.busy": "2026-01-07T15:58:27.756935Z",
     "iopub.status.idle": "2026-01-07T15:58:27.764712Z",
     "shell.execute_reply": "2026-01-07T15:58:27.764039Z"
    },
    "papermill": {
     "duration": 0.011725,
     "end_time": "2026-01-07T15:58:27.766083",
     "exception": false,
     "start_time": "2026-01-07T15:58:27.754358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TRAINING\n",
    "# ==========================================\n",
    "def train_autoencoder():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING \")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print()\n",
    "    \n",
    "    # With aggressive augmentation, model can't memorize exact pixel patterns (they keep changing)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.25, hue=0.1),\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.12, 0.12), scale=(0.88, 1.12)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.25, p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(p=0.35, scale=(0.02, 0.12)),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = VideoFrameDataset(TRAIN_DIR, train_transform)\n",
    "    train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, \n",
    "                             num_workers=2, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    print(f\"Samples: {len(train_dataset)}\\n\")\n",
    "    \n",
    "    model = MultiHeadBottleneck().to(DEVICE)\n",
    "    \n",
    "    criterion = nn.L1Loss(reduction='none')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, \n",
    "                                momentum=0.9, weight_decay=1e-2)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, images in enumerate(train_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            recon, _, attention = model(images)\n",
    "            \n",
    "            pixel_loss = criterion(recon, images)\n",
    "            \n",
    "            #Upsampleing the attention map to match the input image size using bilinear interpolation\n",
    "            attention_upsampled = F.interpolate(attention, size=images.shape[2:], \n",
    "                                               mode='bilinear', align_corners=False)\n",
    "            weighted_loss = (pixel_loss * (1 + attention_upsampled)).mean()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += weighted_loss.item()\n",
    "            \n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"[{epoch+1}/{EPOCHS}][{batch_idx}/{len(train_loader)}] Loss: {weighted_loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}\\n\")\n",
    "    \n",
    "    print(f\"Final: {avg_loss:.4f}\")\n",
    "    print()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e4f7fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T15:58:27.770677Z",
     "iopub.status.busy": "2026-01-07T15:58:27.770354Z",
     "iopub.status.idle": "2026-01-07T15:58:27.784334Z",
     "shell.execute_reply": "2026-01-07T15:58:27.783694Z"
    },
    "papermill": {
     "duration": 0.017879,
     "end_time": "2026-01-07T15:58:27.785625",
     "exception": false,
     "start_time": "2026-01-07T15:58:27.767746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# INFERENCE\n",
    "# ==========================================\n",
    "def run_inference(model):\n",
    "    print(\"=\" * 60)\n",
    "    print(\"INFERENCE \")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Test at multiple scales and average(this improves robusteness):\n",
    "    scales = [80, 88, 96, 104, 112]\n",
    "    print(f\"Scales: {scales}\\n\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    for scale in scales:\n",
    "        print(f\"Scale {scale}...\")\n",
    "        \n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.Resize((scale, scale)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        test_dataset = VideoFrameDataset(TEST_DIR, test_transform, return_id=True)\n",
    "        test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "        \n",
    "        model.eval()\n",
    "        results = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, ids in test_loader:\n",
    "                #If scale is not 96, interpolate to 96x96 for model\n",
    "                if scale != IMAGE_SIZE:\n",
    "                    inputs = F.interpolate(inputs, size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "                                          mode='bilinear', align_corners=False)\n",
    "                \n",
    "                inputs = inputs.to(DEVICE)\n",
    "                outputs, encoded, attention = model(inputs)\n",
    "                \n",
    "                # Core error metrics\n",
    "                l1 = torch.abs(inputs - outputs).mean(dim=[1, 2, 3])\n",
    "                l2 = ((inputs - outputs) ** 2).mean(dim=[1, 2, 3])\n",
    "                max_err = torch.abs(inputs - outputs).amax(dim=[1, 2, 3])\n",
    "                \n",
    "                # Multi-head attention-weighted error\n",
    "                attention_up = F.interpolate(attention, size=inputs.shape[2:], \n",
    "                                            mode='bilinear', align_corners=False)\n",
    "                spatial_err = torch.abs(inputs - outputs) * attention_up\n",
    "                attention_err = spatial_err.mean(dim=[1, 2, 3])\n",
    "                \n",
    "                # Texture variation\n",
    "                std_in = inputs.std(dim=[2, 3]).mean(dim=1)\n",
    "                std_out = outputs.std(dim=[2, 3]).mean(dim=1)\n",
    "                std_err = torch.abs(std_in - std_out)\n",
    "                \n",
    "                # Frequency domain\n",
    "                inputs_freq = torch.fft.rfft2(inputs, dim=[2, 3])\n",
    "                outputs_freq = torch.fft.rfft2(outputs, dim=[2, 3])\n",
    "                freq_err = torch.abs(inputs_freq - outputs_freq).mean(dim=[1, 2, 3])\n",
    "                \n",
    "                # Peak signal analysis (high peaks = anomalies)\n",
    "                peak_in = inputs.amax(dim=[2, 3]).mean(dim=1)\n",
    "                peak_out = outputs.amax(dim=[2, 3]).mean(dim=1)\n",
    "                peak_err = torch.abs(peak_in - peak_out)\n",
    "                \n",
    "                # Optimized weights\n",
    "                scores = (0.26 * l1 + \n",
    "                         0.22 * l2 + \n",
    "                         0.21 * max_err +\n",
    "                         0.15 * attention_err +    # Multi-head attention boost\n",
    "                         0.08 * std_err +\n",
    "                         0.06 * freq_err +\n",
    "                         0.02 * peak_err)          #  peak analysis\n",
    "                \n",
    "                scores = scores.cpu().numpy()\n",
    "                \n",
    "                for id_str, score in zip(ids, scores):\n",
    "                    results.append({'Id': id_str, 'Predicted': float(score)})\n",
    "        \n",
    "        all_predictions.append(pd.DataFrame(results))\n",
    "    \n",
    "    print(f\"\\n Averaging {len(scales)} scales...\")\n",
    "    df = all_predictions[0].copy()\n",
    "    for i in range(1, len(all_predictions)):\n",
    "        df['Predicted'] += all_predictions[i]['Predicted'].values\n",
    "    df['Predicted'] /= len(all_predictions)\n",
    "    \n",
    "    df[['vid', 'frame']] = df['Id'].str.split('_', expand=True).astype(int)\n",
    "    df = df.sort_values(['vid', 'frame'])\n",
    "    \n",
    "    print(\" Applying optimized temporal filter...\")\n",
    "\n",
    "    \n",
    "    filtered_scores = []\n",
    "    for vid in df['vid'].unique():\n",
    "        vid_mask = df['vid'] == vid\n",
    "        vid_scores = df.loc[vid_mask, 'Predicted'].values\n",
    "        \n",
    "       \n",
    "        window_size = min(9, max(5, len(vid_scores) // 15))\n",
    "        if window_size % 2 == 0:\n",
    "            window_size += 1\n",
    "        \n",
    "        smoothed = median_filter(vid_scores, size=window_size, mode='nearest')\n",
    "        filtered_scores.extend(smoothed)\n",
    "    \n",
    "    df['Predicted'] = filtered_scores\n",
    "    df = df.drop(columns=['vid', 'frame']).sort_values('Id')\n",
    "    \n",
    "    scores = df['Predicted'].values\n",
    "    \n",
    "    #Clipping to 0.5-99.5th percentile    \n",
    "    print(\" Clipping to 0.5-99.5th percentile...\")\n",
    "    p05, p995 = np.percentile(scores, [0.5, 99.5])\n",
    "    scores_clipped = np.clip(scores, p05, p995)\n",
    "    \n",
    "    print(f\"\\nFiltered & clipped scores:\")\n",
    "    print(f\"  Min: {scores_clipped.min():.6f}\")\n",
    "    print(f\"  Q25: {np.percentile(scores_clipped, 25):.6f}\")\n",
    "    print(f\"  Q50: {np.percentile(scores_clipped, 50):.6f}\")\n",
    "    print(f\"  Q75: {np.percentile(scores_clipped, 75):.6f}\")\n",
    "    print(f\"  Max: {scores_clipped.max():.6f}\")\n",
    "    \n",
    "    scores_norm = (scores_clipped - scores_clipped.min()) / (scores_clipped.max() - scores_clipped.min())\n",
    "    df['Predicted'] = scores_norm\n",
    "\n",
    "    \n",
    "    df.to_csv(OUTPUT_PATH, index=False)\n",
    "    \n",
    "    print(f\"\\n Saved: {OUTPUT_PATH}\")\n",
    "    print(f\"\\nFirst 20 predictions:\")\n",
    "    print(df.head(20))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "605147a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T15:58:27.789994Z",
     "iopub.status.busy": "2026-01-07T15:58:27.789611Z",
     "iopub.status.idle": "2026-01-07T16:05:50.953050Z",
     "shell.execute_reply": "2026-01-07T16:05:50.952163Z"
    },
    "papermill": {
     "duration": 443.170371,
     "end_time": "2026-01-07T16:05:50.957610",
     "exception": false,
     "start_time": "2026-01-07T15:58:27.787239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING \n",
      "============================================================\n",
      "\n",
      "Samples: 9204\n",
      "\n",
      "[1/8][0/95] Loss: 0.5054\n",
      "[1/8][20/95] Loss: 0.4893\n",
      "[1/8][40/95] Loss: 0.4508\n",
      "[1/8][60/95] Loss: 0.4130\n",
      "[1/8][80/95] Loss: 0.3916\n",
      "Epoch 1 - Loss: 0.4444\n",
      "\n",
      "[2/8][0/95] Loss: 0.3733\n",
      "[2/8][20/95] Loss: 0.3385\n",
      "[2/8][40/95] Loss: 0.3325\n",
      "[2/8][60/95] Loss: 0.3020\n",
      "[2/8][80/95] Loss: 0.2840\n",
      "Epoch 2 - Loss: 0.3155\n",
      "\n",
      "[3/8][0/95] Loss: 0.2780\n",
      "[3/8][20/95] Loss: 0.2581\n",
      "[3/8][40/95] Loss: 0.2418\n",
      "[3/8][60/95] Loss: 0.2331\n",
      "[3/8][80/95] Loss: 0.2262\n",
      "Epoch 3 - Loss: 0.2414\n",
      "\n",
      "[4/8][0/95] Loss: 0.2154\n",
      "[4/8][20/95] Loss: 0.2207\n",
      "[4/8][40/95] Loss: 0.2037\n",
      "[4/8][60/95] Loss: 0.2033\n",
      "[4/8][80/95] Loss: 0.1978\n",
      "Epoch 4 - Loss: 0.2035\n",
      "\n",
      "[5/8][0/95] Loss: 0.1898\n",
      "[5/8][20/95] Loss: 0.1850\n",
      "[5/8][40/95] Loss: 0.1863\n",
      "[5/8][60/95] Loss: 0.1831\n",
      "[5/8][80/95] Loss: 0.1791\n",
      "Epoch 5 - Loss: 0.1830\n",
      "\n",
      "[6/8][0/95] Loss: 0.1727\n",
      "[6/8][20/95] Loss: 0.1751\n",
      "[6/8][40/95] Loss: 0.1647\n",
      "[6/8][60/95] Loss: 0.1727\n",
      "[6/8][80/95] Loss: 0.1662\n",
      "Epoch 6 - Loss: 0.1697\n",
      "\n",
      "[7/8][0/95] Loss: 0.1644\n",
      "[7/8][20/95] Loss: 0.1599\n",
      "[7/8][40/95] Loss: 0.1605\n",
      "[7/8][60/95] Loss: 0.1552\n",
      "[7/8][80/95] Loss: 0.1616\n",
      "Epoch 7 - Loss: 0.1596\n",
      "\n",
      "[8/8][0/95] Loss: 0.1545\n",
      "[8/8][20/95] Loss: 0.1555\n",
      "[8/8][40/95] Loss: 0.1538\n",
      "[8/8][60/95] Loss: 0.1517\n",
      "[8/8][80/95] Loss: 0.1452\n",
      "Epoch 8 - Loss: 0.1515\n",
      "\n",
      "Final: 0.1515\n",
      "\n",
      "============================================================\n",
      "INFERENCE \n",
      "============================================================\n",
      "Scales: [80, 88, 96, 104, 112]\n",
      "\n",
      "Scale 80...\n",
      "Scale 88...\n",
      "Scale 96...\n",
      "Scale 104...\n",
      "Scale 112...\n",
      "\n",
      " Averaging 5 scales...\n",
      " Applying optimized temporal filter...\n",
      " Clipping to 0.5-99.5th percentile...\n",
      "\n",
      "Filtered & clipped scores:\n",
      "  Min: 0.587015\n",
      "  Q25: 0.603006\n",
      "  Q50: 0.610965\n",
      "  Q75: 0.624473\n",
      "  Max: 0.748068\n",
      "\n",
      " Saved: /kaggle/working/submission.csv\n",
      "\n",
      "First 20 predictions:\n",
      "          Id  Predicted\n",
      "6020  10_100   0.289758\n",
      "6021  10_101   0.292367\n",
      "6022  10_102   0.309175\n",
      "6023  10_103   0.339736\n",
      "6024  10_104   0.340957\n",
      "6025  10_105   0.340957\n",
      "6026  10_106   0.340957\n",
      "6027  10_107   0.340957\n",
      "6028  10_108   0.339736\n",
      "6029  10_109   0.332890\n",
      "6030  10_110   0.332624\n",
      "6031  10_111   0.332624\n",
      "6032  10_112   0.332624\n",
      "6033  10_113   0.332624\n",
      "6034  10_114   0.332890\n",
      "6035  10_115   0.334404\n",
      "6036  10_116   0.350344\n",
      "6037  10_117   0.350344\n",
      "6038  10_118   0.360055\n",
      "6039  10_119   0.360055\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = train_autoencoder()\n",
    "    submission = run_inference(model)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 15067517,
     "sourceId": 126766,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 459.237064,
   "end_time": "2026-01-07T16:05:53.988593",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-07T15:58:14.751529",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
